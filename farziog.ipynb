{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install opencv-python-headless scikit-image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "dataset_paths_real = {\n",
    "    '2000': '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_dataset',\n",
    "    '500': '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_dataset'\n",
    "}\n",
    "\n",
    "dataset_paths_fake = {\n",
    "    '2000': '/content/drive/MyDrive/farzi_git_sayan/Fake Notes/2000',\n",
    "    '500': '/content/drive/MyDrive/farzi_git_sayan/Fake Notes/500'\n",
    "}\n",
    "feature_paths = {\n",
    "    '2000': [\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 1',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 2',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 3',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 4',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 5',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 6',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/2000_Features Dataset/Feature 7'\n",
    "    ],\n",
    "    '500': [\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 1',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 2',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 3',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 4',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 5',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 6',\n",
    "        '/content/drive/MyDrive/farzi_git_sayan/Dataset/500_Features Dataset/Feature 7'\n",
    "    ]\n",
    "}\n",
    "def load_image(image_path):\n",
    "    \"\"\"Load an image from the specified path.\"\"\"\n",
    "    return cv2.imread(image_path)\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Convert image to grayscale, resize, and apply Gaussian blur.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (300, 300))  # Resize to a fixed size\n",
    "    blurred = cv2.GaussianBlur(resized, (5, 5), 0)\n",
    "    return blurred\n",
    "def extract_features(image):\n",
    "    \"\"\"Extract ORB features from the image.\"\"\"\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "def match_features(des1, des2):\n",
    "    \"\"\"Match features between two sets of descriptors.\"\"\"\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    return matches\n",
    "def compute_similarity(image1, image2):\n",
    "    \"\"\"Compute similarity score between two images based on feature matches.\"\"\"\n",
    "    gray1 = preprocess_image(image1)\n",
    "    gray2 = preprocess_image(image2)\n",
    "    kp1, des1 = extract_features(gray1)\n",
    "    kp2, des2 = extract_features(gray2)\n",
    "    matches = match_features(des1, des2)\n",
    "    similarity_score = len(matches)\n",
    "    return similarity_score\n",
    "def extract_features_from_images(image_paths, label_map):\n",
    "    \"\"\"Extract features and labels from a list of image paths.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for image_path in image_paths:\n",
    "        image = load_image(image_path)\n",
    "        gray = preprocess_image(image)\n",
    "        kp, des = extract_features(gray)\n",
    "        if des is not None:\n",
    "            feature_vector = np.mean(des, axis=0)  # Averaging descriptors\n",
    "            features.append(feature_vector)\n",
    "            label = label_map.get(os.path.basename(image_path).split('_')[0], 0)\n",
    "            labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train and evaluate a Random Forest model.\"\"\"\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return clf\n",
    "def save_model(clf, model_path):\n",
    "    \"\"\"Save the trained model to a file.\"\"\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"Calculate Structural Similarity Index (SSIM) between two images.\"\"\"\n",
    "    return ssim(img1, img2, data_range=img2.max() - img2.min())\n",
    "def test_feature(dataset_name, image_size):\n",
    "    \"\"\"Test feature extraction and template matching for a given dataset and image size.\"\"\"\n",
    "    dataset_paths = feature_paths[dataset_name]\n",
    "    test_img_path = os.path.join(dataset_paths_real[dataset_name], f'{image_size}_s1.jpg')\n",
    "    \n",
    "    score_set_list = []\n",
    "    best_extracted_img_list = []\n",
    "    avg_ssim_list = []\n",
    "\n",
    "    for j in range(len(dataset_paths)):\n",
    "        print(f'ANALYSIS OF FEATURE {j+1}')\n",
    "        score_set = []\n",
    "        max_score = -1\n",
    "        max_score_img = None\n",
    "\n",
    "        # Process exactly 6 templates\n",
    "        for i in range(1, 7):\n",
    "            print(f'---> Template {i} :')\n",
    "            template_path = os.path.join(dataset_paths[j], f'{i}.jpg')\n",
    "            template_img = cv2.imread(template_path)\n",
    "            if template_img is None:\n",
    "                print(f\"Error: Unable to load template image from {template_path}\")\n",
    "                continue\n",
    "            \n",
    "            template_img_gray = preprocess_image(template_img)\n",
    "\n",
    "            # Load and preprocess the test image\n",
    "            test_img = cv2.imread(test_img_path)\n",
    "            if test_img is None:\n",
    "                print(f\"Error: Unable to load test image from {test_img_path}\")\n",
    "                continue\n",
    "\n",
    "            test_img_gray = preprocess_image(test_img)\n",
    "            \n",
    "            # Perform feature matching\n",
    "            kp1, des1 = extract_features(template_img_gray)\n",
    "            kp2, des2 = extract_features(test_img_gray)\n",
    "            if des1 is None or des2 is None:\n",
    "                continue\n",
    "            matches = match_features(des1, des2)\n",
    "            \n",
    "            # Draw matches\n",
    "            matches_img = cv2.drawMatches(template_img_gray, kp1, test_img_gray, kp2, matches[:20], None, flags=2)\n",
    "            plt.imshow(matches_img)\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate average SSIM\n",
    "            avg_ssim = calculate_ssim(template_img_gray, test_img_gray)\n",
    "            avg_ssim_list.append(avg_ssim)\n",
    "            score_set.append(avg_ssim)\n",
    "            if avg_ssim > max_score:\n",
    "                max_score = avg_ssim\n",
    "                max_score_img = template_path\n",
    "\n",
    "        best_extracted_img_list.append(max_score_img)\n",
    "        score_set_list.append(score_set)\n",
    "        print(f'Best template for feature {j+1} is {max_score_img} with score: {max_score:.2f}')\n",
    "\n",
    "    return best_extracted_img_list, score_set_list, avg_ssim_list\n",
    "def main():\n",
    "    real_image_paths = [os.path.join(path, f) for path in dataset_paths_real.values() for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    fake_image_paths = [os.path.join(path, f) for path in dataset_paths_fake.values() for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "    all_image_paths = real_image_paths + fake_image_paths\n",
    "    labels = [1] * len(real_image_paths) + [0] * len(fake_image_paths)  # 1 for real, 0 for fake\n",
    "    dummy_label_map = {os.path.basename(path).split('_')[0]: 1 for path in all_image_paths}\n",
    "    features, labels = extract_features_from_images(all_image_paths, label_map=dummy_label_map)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "    clf = train_and_evaluate_model(X_train, X_test, y_train, y_test)\n",
    "    model_path = '/content/drive/MyDrive/farzi_git_sayan/farzimodel.pkl'\n",
    "    save_model(clf, model_path)\n",
    "    for dataset in ['2000', '500']:\n",
    "        print(f'\\nTesting feature extraction for dataset {dataset}:')\n",
    "        for size in ['2000', '500']:  # Test both sizes if needed\n",
    "            best_imgs, score_sets, avg_ssims = test_feature(dataset, size)\n",
    "            print(f\"Image size: {size}\")\n",
    "            print(\"Best images:\", best_imgs)\n",
    "            print(\"Score sets:\", score_sets)\n",
    "            print(\"Average SSIMs:\", avg_ssims)\n",
    "    clf = load_model(model_path)\n",
    "    test_image_path = '/content/drive/MyDrive/farzi_git_sayan/Real.jpg'\n",
    "    result = classify_image(test_image_path, clf)\n",
    "    if result is not None:\n",
    "        print(f'The image at {test_image_path} is classified as: {result}')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
